

optimizer_name: Adam       # Optimizer type (e.g., Adam, SGD, AdamW)

lr: 0.0001          # Initial learning rate

weight_decay: 0.000001   # Weight decay (L2 regularization)